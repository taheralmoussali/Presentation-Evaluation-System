{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5xBHXnh7SLrA",
      "metadata": {
        "id": "5xBHXnh7SLrA"
      },
      "source": [
        "## install needed libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8xiLbOhi3N8B",
      "metadata": {
        "id": "8xiLbOhi3N8B"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rl_P5e2ByQR-",
      "metadata": {
        "id": "rl_P5e2ByQR-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import argparse, os, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from colour import Color\n",
        "import easydict\n",
        "from IPython.display import clear_output\n",
        "import mediapipe as mp\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "\n",
        "import os \n",
        "from numpy.lib.function_base import average"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A4wEpwKcryGf",
      "metadata": {
        "id": "A4wEpwKcryGf"
      },
      "source": [
        "## Deep eye contact model \n",
        "Deep neural network trained to detect eye contact from facial image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64709dc5",
      "metadata": {
        "id": "64709dc5"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def model_static(pretrained=False, **kwargs):\n",
        "    model = ResNet([3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        print ('loading saved model weights')\n",
        "        model_dict = model.state_dict()\n",
        "        snapshot = torch.load(f = pretrained, map_location=torch.device('cuda'))\n",
        "        snapshot = {k: v for k, v in snapshot.items() if k in model_dict}\n",
        "        model_dict.update(snapshot)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3,\n",
        "                               bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer1 = self._make_layer(64, layers[0])\n",
        "        self.layer2 = self._make_layer(128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride = 1)\n",
        "        self.fc_theta = nn.Linear(512 * Bottleneck.expansion, 34)\n",
        "        self.fc_phi = nn.Linear(512 * Bottleneck.expansion, 34)\n",
        "        self.fc_ec = nn.Linear(512 * Bottleneck.expansion, 1)\n",
        "        self.init_param()\n",
        "\n",
        "    def init_param(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.shape[0] * m.weight.shape[1]\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride = 1):\n",
        "        downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * Bottleneck.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * Bottleneck.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * Bottleneck.expansion),\n",
        "                )\n",
        "\n",
        "        layers.append(Bottleneck(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * Bottleneck.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(Bottleneck(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_ec(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcb15e3d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "OnSBLnCTBP0r",
      "metadata": {
        "id": "OnSBLnCTBP0r"
      },
      "source": [
        "## the main method \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ce413e",
      "metadata": {
        "id": "23ce413e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def detect_eye_contact(video_path, model_weight, vis, display_off, save_text , output_path):\n",
        "    \"\"\"\n",
        "    input: \n",
        "    video_path: The video path you want\n",
        "    model_weight: the path of this \"model_weights.pkl\"\n",
        "    vis: True or False to save output video\n",
        "    display_off: True or False to display output video\n",
        "    save_text: True or False to save output file contains all score of frames\n",
        "    output_path: the path for save output file \n",
        "\n",
        "    output:\n",
        "    score: score of eye contact\n",
        "    confidence: confidence of the taken score\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    num_person = video_path.split('/')[-1].split(\" \")[1].split(\"-\")[0]\n",
        "    name_class = video_path.split('-')[-1].split(\".\")[0]\n",
        "\n",
        "    #for using face detection from mediapip library\n",
        "    mp_face_detection = mp.solutions.face_detection\n",
        "\n",
        "    # set up vis settings\n",
        "    red = Color(\"red\")\n",
        "    colors = list(red.range_to(Color(\"green\"),10))\n",
        "    font = ImageFont.truetype(\"/content/drive/MyDrive/data/arial.ttf\", 40)\n",
        "    frames = []\n",
        "\n",
        "    # O is set contains list of y and confidence for every frame\n",
        "    # y is 1 if exist eye contact in frame and 0 if not \n",
        "    # confidence is the confidence in frame \n",
        "    O = {\"y\":[] , \"confidence\" :[]}\n",
        "    # k is list contains confidence then more 50%\n",
        "    K = []\n",
        "\n",
        "    # set up video source\n",
        "    if video_path is None:\n",
        "        print(\"we don't have video for display\")\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # set up output file\n",
        "    if save_text:\n",
        "        outtext_name = os.path.basename(video_path).replace('.mp4','_output.csv')\n",
        "        # folder = video_path.split('/')[-1].split(\" \")[1].split(\"-\")[0]\n",
        "        folder = video_path.split('/')[-2].split(\"/\")[0]\n",
        "        path = os.path.join(output_path, folder)\n",
        "        if os.path.isdir(path):\n",
        "          print(path)\n",
        "        else:\n",
        "          os.mkdir(path)\n",
        "        outtext_name = os.path.join(path, outtext_name)\n",
        "        print(outtext_name)\n",
        "        f = open(outtext_name, \"w\")\n",
        "\n",
        "    if vis:\n",
        "        outvis_name = os.path.basename(video_path).replace('.mp4','_output.avi')\n",
        "        outvis_name = os.path.join(path, outvis_name)\n",
        "\n",
        "        # We need to set resolutions.\n",
        "        # so, convert them from float to integer.\n",
        "        imwidth = int(cap.get(3)); imheight = int(cap.get(4))\n",
        "      \n",
        "       \n",
        "        # Below VideoWriter object will create\n",
        "        # a frame of above defined The output \n",
        "        # is stored in outvis file.\n",
        "        outvid = cv2.VideoWriter(outvis_name,cv2.VideoWriter_fourcc(*'MJPG'), 10, (imwidth,imheight))\n",
        "\n",
        "  \n",
        "    if (cap.isOpened()== False):\n",
        "        print(\"Error opening video stream or file\")\n",
        "        exit()\n",
        "\n",
        "    frame_cnt = 0\n",
        "\n",
        "    # set up data transformation\n",
        "    test_transforms = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    # load model weights\n",
        "    model = model_static(model_weight)\n",
        "    model_dict = model.state_dict()\n",
        "    snapshot = torch.load(model_weight, map_location=torch.device('cuda'))\n",
        "    # snapshot = torch.load(model_weight)\n",
        "    model_dict.update(snapshot)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    model.cuda()\n",
        "    model.train(False)\n",
        "\n",
        "    # video reading loop\n",
        "    while(cap.isOpened()):\n",
        "        with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
        "\n",
        "          ret, frame = cap.read()\n",
        "          if ret == True:\n",
        "              height, width, channels = frame.shape\n",
        "              frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "              frame_cnt += 1\n",
        "              bbox = []\n",
        "\n",
        "          #instead of Dlib \n",
        "              results = face_detection.process(frame)\n",
        "\n",
        "              if results.detections:\n",
        "                for detection in results.detections:\n",
        "\n",
        "                  results.detections[0].location_data.relative_bounding_box.xmin\n",
        "\n",
        "                  l , t = normaliz_pixel(detection.location_data.relative_bounding_box.xmin ,detection.location_data.relative_bounding_box.ymin , width, height )\n",
        "                  r = detection.location_data.relative_bounding_box.width*width + l\n",
        "                  b = detection.location_data.relative_bounding_box.height*height + t\n",
        "\n",
        "                  l -= (r-l)*0.2\n",
        "                  r += (r-l)*0.2\n",
        "                  t -= (b-t)*0.2\n",
        "                  b += (b-t)*0.2\n",
        "\n",
        "                  bbox.append([l,t,r,b])\n",
        "                \n",
        "            \n",
        "              frame = Image.fromarray(frame)\n",
        "              for b in bbox:\n",
        "                  face = frame.crop((b))\n",
        "                  img = test_transforms(face)\n",
        "                  img.unsqueeze_(0)\n",
        "                  \n",
        "                  # forward pass\n",
        "                  output = model(img.cuda())\n",
        "\n",
        "                  score = F.sigmoid(output).item()\n",
        "\n",
        "                  # 0 -- 1 \n",
        "                  # 0.9 > eye contact \n",
        "                  # 0.9 < without eye contact\n",
        "\n",
        "                  O[\"confidence\"].append(score)\n",
        "                  y = 1 if score>=0.9 else 0\n",
        "                  O[\"y\"].append(y)\n",
        "                  if score>0.5:\n",
        "                    K.append(score)\n",
        "\n",
        "                  coloridx = min(int(round(score*10)),9)\n",
        "                  draw = ImageDraw.Draw(frame)\n",
        "                  draw_rectangle(draw, [(b[0], b[1]), (b[2], b[3])], outline=colors[coloridx].hex, width=5)\n",
        "                  draw.text((b[0],b[3]), str(round(score,2)), fill=(255,255,255,128), font=font)\n",
        "                  if y == 1:\n",
        "                    draw.text((10, 60), \"eye contact\", font=font, fill=(255, 255, 255, 255))\n",
        "                  else:\n",
        "                    draw.text((10, 60), \"without eye contact\", font=font, fill=(255, 255, 255, 255))\n",
        "                  if save_text:\n",
        "                      # we will store number frame and decision with eye contact or not\n",
        "                      f.write(\"%d,%f\\n\"%(frame_cnt,y))\n",
        "\n",
        "              \n",
        "              frame = np.asarray(frame) # convert PIL image back to opencv format for faster display\n",
        "              frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                  # dim = (600, 335)\n",
        "                  # resized=cv2.resize(frame,dim)\n",
        "              if not display_off:\n",
        "                  clear_output(wait=True)\n",
        "                  # cv2_imshow(resized)\n",
        "                  \n",
        "               # Display on colab\n",
        "                  cv2_imshow(frame)\n",
        "\n",
        "               # Display on Jupyter\n",
        "                  # cv2.imshow('',frame)\n",
        "                  \n",
        "                  key = cv2.waitKey(1) & 0xFF\n",
        "                  if key == ord('q'):\n",
        "                      break\n",
        "              if vis:\n",
        "                  outvid.write(frame)\n",
        "                  \n",
        "          else:\n",
        "              break\n",
        "\n",
        "    confidence = len(K)/len(O[\"y\"]) \n",
        "    score = average(K)\n",
        "    if score>0.85 and confidence>0.75:\n",
        "      final_result = 1\n",
        "    else:\n",
        "      final_result = 0\n",
        "\n",
        "    if save_text:\n",
        "        f.write(\"score of eye contact: %f ,confidence : %f\\n\"%(score,confidence))\n",
        "        print(video_path,\"score of eye contact: %f ,confidence : %f\\n\"%(average(K),confidence))\n",
        "        f.close()\n",
        "\n",
        "    cap.release()\n",
        "    if vis:\n",
        "        outvid.release()\n",
        "    print ('DONE!')\n",
        "\n",
        "    if num_person in the_results[\"number person\"]:\n",
        "        print(\"exist\")\n",
        "    else:\n",
        "      the_results[\"number person\"].append(num_person)\n",
        "    # the_results[name_class][0].append(average(K))\n",
        "    # the_results[name_class][1].append(confidence)\n",
        "    \n",
        "    return final_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ib-gqmhoRUca",
      "metadata": {
        "id": "ib-gqmhoRUca"
      },
      "source": [
        "## to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q6kZh8bTtDw1",
      "metadata": {
        "id": "q6kZh8bTtDw1"
      },
      "outputs": [],
      "source": [
        "the_results = {\"number person\":[] ,\n",
        "               \"A: score\": [],\"A: confidence\": [],\n",
        "               \"B: score\": [],\"B: confidence\": [],\n",
        "               \"C: score\": [],\"C: confidence\": [],\n",
        "               \"D: score\": [],\"D: confidence\": [],}\n",
        "\n",
        "# pass vedio path \n",
        "groupC = '/content/drive/MyDrive/dataset of presentation scoring/Presenter No1-Asylah/C_presenter No1-groupB.mp4'\n",
        "\n",
        "# args = parser.parse_args()\n",
        "# save text: will be false or true\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "    \"video\": groupC,\n",
        "    \"model_weight\": \"/content/drive/MyDrive/data/model_weights.pkl\",\n",
        "    \"save_vis\": False,\n",
        "    \"save_text\": True,\n",
        "    \"display_off\": True \n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hFnq-gYklgkt",
      "metadata": {
        "id": "hFnq-gYklgkt"
      },
      "source": [
        "Execute on one video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe0fd81",
      "metadata": {
        "id": "3fe0fd81"
      },
      "outputs": [],
      "source": [
        "detect_eye_contact(args.video,args.model_weight, args.save_vis, args.display_off, args.save_text , \"/content/drive/MyDrive/The results of eye contact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ILvo-nPwl4gL",
      "metadata": {
        "id": "ILvo-nPwl4gL"
      },
      "source": [
        "Execute on many videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IeVa9Szil4gL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeVa9Szil4gL",
        "outputId": "d9114ecc-4809-4aba-8204-5e1608a665da"
      },
      "outputs": [],
      "source": [
        "# for run and processing more than one video simutanously in videos:\n",
        "\n",
        "videos = get_videos(\"/content/drive/MyDrive/dataset of presentation scoring\")\n",
        "for video in videos:\n",
        "  ##\n",
        "  ## this code to prevent execution again\n",
        "  outtext_name = os.path.basename(video).replace('.mp4','_output.csv')\n",
        "  folder = video.split('/')[-2].split(\"/\")[0]\n",
        "  output_path = '/content/drive/MyDrive/The results of eye contact/supervised'\n",
        "  path = os.path.join(output_path, folder)\n",
        "  outtext_name = os.path.join(path, outtext_name)\n",
        "  print(outtext_name)\n",
        "  if os.path.isfile(outtext_name):\n",
        "    print(\"already is runing\")\n",
        "    pass\n",
        "  ##\n",
        "  else:\n",
        "    detect_eye_contact(video,args.model_weight, args.save_vis, args.display_off, args.save_text ,\n",
        "                     \"/content/drive/MyDrive/The results of eye contact/supervised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O2nJWiwlICyx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2nJWiwlICyx",
        "outputId": "c1e8a4c1-d8cf-4a79-ff68-792982ed5a24"
      },
      "outputs": [],
      "source": [
        "\n",
        "# the path of the dataset on the Drive\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Eye contact data \"\n",
        "folders ,videos = get_videos(data_path)\n",
        "print(folders)\n",
        "videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RNLp1jz-IQTk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "RNLp1jz-IQTk",
        "outputId": "d26ed0e7-8343-4241-eed4-e762c5fdc617"
      },
      "outputs": [],
      "source": [
        "# convert the results dictionary to dataframe \n",
        "df = pd.DataFrame(the_results)\n",
        "import os  \n",
        "os.makedirs('/content/drive/MyDrive/', exist_ok=True)  \n",
        "df.to_csv('/content/drive/MyDrive/result_of_eye_contact.csv')  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gW3pt6acJZ17",
      "metadata": {
        "id": "gW3pt6acJZ17"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9kJG6fjPJZyS",
      "metadata": {
        "id": "9kJG6fjPJZyS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k44CHNWxJZvf",
      "metadata": {
        "id": "k44CHNWxJZvf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150v7n5UJZs8",
      "metadata": {
        "id": "150v7n5UJZs8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_hNXrUOvkyAP",
      "metadata": {
        "id": "_hNXrUOvkyAP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oM5o_pxDar5n",
      "metadata": {
        "id": "oM5o_pxDar5n"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import time\n",
        "import mediapipe as mp\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# For static images:\n",
        "# IMAGE_FILES = []\n",
        "# with mp_face_detection.FaceDetection(\n",
        "#     model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
        "#   for idx, file in enumerate(IMAGE_FILES):\n",
        "#     image = cv2.imread(file)\n",
        "#     # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
        "#     results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#     # Draw face detections of each face.\n",
        "#     if not results.detections:\n",
        "#       continue\n",
        "#     annotated_image = image.copy()\n",
        "#     for detection in results.detections:\n",
        "#       print('Nose tip:')\n",
        "#       print(mp_face_detection.get_key_point(\n",
        "#           detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
        "#       mp_drawing.draw_detection(annotated_image, detection)\n",
        "#     cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n",
        "\n",
        "\n",
        "# For webcam input:\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/test_eye_c.mp4\")\n",
        "with mp_face_detection.FaceDetection(\n",
        "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
        "  while cap.isOpened():\n",
        "    success, image = cap.read()\n",
        "    if not success:\n",
        "      print(\"Ignoring empty camera frame.\")\n",
        "      # If loading a video, use 'break' instead of 'continue'.\n",
        "      continue\n",
        "\n",
        "    # To improve performance, optionally mark the image as not writeable to\n",
        "    # pass by reference.\n",
        "    image.flags.writeable = False\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = face_detection.process(image)\n",
        "\n",
        "    print(results)\n",
        "    # Draw the face detection annotations on the image.\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    img_h, img_w=image.shape[:2]\n",
        "\n",
        "    # mesh_points =np.array([np.multiply([p.x,p.y],[img_w,img_h]).astype(int) for p in results.multi_face_landmarks[0].landmark])\n",
        "    if results.detections:\n",
        "      print(results.detections[0].location_data.relative_bounding_box.xmin)\n",
        "      time.sleep(0.5)\n",
        "      for detection in results.detections:\n",
        "        mp_drawing.draw_detection(image, detection)\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    # clear_output(wait=True)\n",
        "    # cv2_imshow(image)\n",
        "    # cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
        "    if cv2.waitKey(5) & 0xFF == 27:\n",
        "      break\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u5U0zKyh0m-m",
      "metadata": {
        "id": "u5U0zKyh0m-m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5xBHXnh7SLrA",
        "8xiLbOhi3N8B",
        "A4wEpwKcryGf"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
